{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mert Arcan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Rain in Austuralia Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather in Australia dataset: https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python version: Python 3.10.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import scatter_matrix\n",
    "weatherAUS = pd.read_csv(\"weatherAUS.csv\")\n",
    "num_Columns = weatherAUS.select_dtypes(include=\"float64\").columns\n",
    "cat_Columns = weatherAUS.select_dtypes(include=\"object\").columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(dataFrame):\n",
    "    df = dataFrame\n",
    "    # Drop the values that target variable is NaN\n",
    "    df=df[(df[\"RainTomorrow\"]==\"Yes\")|(df[\"RainTomorrow\"]==\"No\")]\n",
    "    # For the rest of Categorical Variables, use the mod of that column to replace NaN's\n",
    "    df['WindGustDir'] = df['WindGustDir'].fillna(df['WindGustDir'].mode()[0])\n",
    "    df['WindDir9am'] = df['WindDir9am'].fillna(df['WindDir9am'].mode()[0])\n",
    "    df['WindDir3pm'] = df['WindDir3pm'].fillna(df['WindDir3pm'].mode()[0])\n",
    "    df['RainToday'] = df['RainToday'].fillna(df['RainToday'].mode()[0])\n",
    "\n",
    "\n",
    "\n",
    "    la = LabelEncoder()\n",
    "    l = []\n",
    "    for i in df.columns:\n",
    "        if df.dtypes[i]=='O':\n",
    "            l.append(i)\n",
    "    for i in l:\n",
    "        print(i)\n",
    "        df[i] = la.fit_transform(df[i])\n",
    "        \n",
    "    # For the rest numeric values, take the mean of these columns to replace NaN's\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputer = imputer.fit(df)\n",
    "    df = pd.DataFrame(imputer.transform(df), columns= df.columns, index = df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def removeOutliers(dataFrame,outlier_features):\n",
    "    df = dataFrame\n",
    "    for x in outlier_features:\n",
    "        q25=np.quantile(df[x],0.25)\n",
    "        q75=np.quantile(df[x],0.75)\n",
    "        iqr=q75-q25\n",
    "        lower=round(q25-1.5*iqr,2)\n",
    "        upper=round(q75+1.5*iqr,2)\n",
    "        print(x,\": Upper: \",upper,\" || Lower: \",lower)\n",
    "        df=df[(df[x]<upper) & (df[x]>lower)]\n",
    "    return df\n",
    "\n",
    "def minMax_scale(dataFrame):\n",
    "    df = dataFrame\n",
    "    minmaxScaler = MinMaxScaler()\n",
    "    scaled = minmaxScaler.fit_transform(df)\n",
    "    df = pd.DataFrame(scaled,columns= df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale(X_train,X_test):\n",
    "    xtrain = X_train\n",
    "    xtest = X_test\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    xtrain = scaler.transform(xtrain)\n",
    "    xtest = scaler.transform(xtest)\n",
    "    xtrain = pd.DataFrame(xtrain,columns = X_train.columns, index = X_train.index)\n",
    "    xtest = pd.DataFrame(xtest,columns = X_test.columns, index=X_test.index)\n",
    "    return xtrain,xtest\n",
    "\n",
    "def my_normalize(dataFrame):\n",
    "    df = dataFrame\n",
    "    normalizer = Normalizer()\n",
    "    normalized = normalizer.fit_transform(df)\n",
    "    normalized_df = pd.DataFrame(normalized,columns= df.columns)\n",
    "    return normalized_df\n",
    "\n",
    "def checkNaN(dataFrame):\n",
    "    for i in dataFrame.columns:\n",
    "        if(dataFrame[i].isnull().values.any()):\n",
    "            count = dataFrame[i].isnull().sum()\n",
    "            print(i,\", count of NaNs:\",count)\n",
    "        else:\n",
    "            print(\"No NaN Values in \",i)\n",
    "            \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherAUS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherAUS.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scatter_matrix(weatherAUS,alpha = 0.1, figsize=(20,12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(weatherAUS[\"Temp3pm\"],weatherAUS[\"Temp9am\"], alpha = 0.1)\n",
    "plt.title(\"Temp3pm / Temp9am\")\n",
    "plt.xlabel(\"Temp3pm\")\n",
    "plt.ylabel(\"Temp9am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(weatherAUS[\"Pressure3pm\"],weatherAUS[\"Pressure9am\"], alpha = 0.1)\n",
    "plt.title(\"Pressure3pm / Pressure9am\")\n",
    "plt.xlabel(\"Pressure3pm\")\n",
    "plt.ylabel(\"Pressure9am\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(weatherAUS[\"MinTemp\"],weatherAUS[\"MaxTemp\"], alpha = 0.1)\n",
    "plt.title(\"MinTemp / MaxTemp\")\n",
    "plt.xlabel(\"MinTemp\")\n",
    "plt.ylabel(\"MaxTemp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherAUS[\"Temp_diff\"] = weatherAUS[\"Temp3pm\"] - weatherAUS[\"Temp9am\"]\n",
    "weatherAUS[\"Pressure_diff\"] = weatherAUS[\"Pressure3pm\"] - weatherAUS[\"Pressure9am\"]\n",
    "weatherAUS[\"MinMaxTemp_diff\"] = weatherAUS[\"MaxTemp\"] - weatherAUS[\"MinTemp\"]\n",
    "weatherAUS.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkNaN(weatherAUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherAUS = weatherAUS.drop(\"Date\", axis = 1)\n",
    "weatherAUS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set_Org, test_set_Org = train_test_split(weatherAUS,test_size=0.2, random_state=442)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_Org.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = preProcess(train_set_Org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_Org.head().T # Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head().T # Preprocessed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.describe().T\n",
    "# Rainfall\n",
    "# Evaporation\n",
    "# WindGustSpeed\n",
    "# WindSpeed9am\n",
    "# WindSpeed3pm\n",
    "# may contain outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "outliers.append(\"Rainfall\")\n",
    "outliers.append(\"Evaporation\")\n",
    "outliers.append(\"WindSpeed3pm\")\n",
    "outliers.append(\"WindSpeed9am\")\n",
    "outliers.append(\"WindGustSpeed\")\n",
    "sns.set(rc = {'figure.figsize':(10,5)})\n",
    "train_set[outliers].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = removeOutliers(train_set,outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[outliers].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['MinTemp', 'MaxTemp','Pressure9am', 'Pressure3pm','Temp9am', 'Temp3pm','RainTomorrow']\n",
    "train_set_corr = train_set[col].corr()\n",
    "#train_set_corr[\"RainTomorrow\"].sort_values(ascending= False)\n",
    "sns.heatmap(train_set_corr,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_diff = [\"MinMaxTemp_diff\", \"Pressure_diff\", \"Temp_diff\",\"RainTomorrow\"]\n",
    "train_set_diff_corr = train_set[col_diff].corr()\n",
    "sns.heatmap(train_set_diff_corr,annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcol = ['MinTemp', 'MaxTemp','Pressure9am', 'Pressure3pm','Temp9am', 'Temp3pm']\n",
    "test_set = preProcess(test_set_Org)\n",
    "test_set = removeOutliers(test_set,outliers)\n",
    "train_set = train_set.drop(dropcol, axis = 1)\n",
    "test_set = test_set.drop(dropcol, axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_set.drop(\"RainTomorrow\", axis = 1)\n",
    "X_train = train_set.drop(\"RainTomorrow\", axis = 1)\n",
    "y_train = train_set.RainTomorrow\n",
    "y_test = test_set.RainTomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_set.corr()\n",
    "corr[\"RainTomorrow\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = scale(X_train = X_train,X_test= X_test)\n",
    "X_train.hist(bins= 50, figsize=(20,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def printScores(scores,name):\n",
    "    print(\"Scores for -->\",name)\n",
    "    print(\"Scores:\",scores)\n",
    "    print(\"Mean (%):\",scores.mean()*100)\n",
    "    print(\"Scores Mean:\",scores.mean())\n",
    "    print(\"Scores std:\",scores.std())\n",
    "    print(\"Scores std:(%)\",scores.std()*100)\n",
    "\n",
    "\n",
    "def compareModels(cNames,classifiers,X,Y):\n",
    "    for c in range(len(classifiers)):\n",
    "        model = classifiers[c]\n",
    "        scores = cross_val_score(model,X,Y,scoring=\"accuracy\",cv = 10)\n",
    "        printScores(scores,cNames[c])\n",
    "        print(\"\\n-------------------------------------------------------\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "names = [\n",
    "    \n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Multi-layer Perceptron\",\n",
    "    \"Gaussian Naive Bayes\",\n",
    "\n",
    "]\n",
    "classifiers =[\n",
    "    \n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    MLPClassifier(alpha = 1),\n",
    "    GaussianNB(),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareModels(names,classifiers,X= X_train, Y= y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions) * 100\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 60, stop = 500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [2,4,6,8,10,12,14,16,18,20]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "rf_random.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = RandomForestClassifier()\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "param_grid = [\n",
    "    \n",
    "    {'bootstrap': [True],'n_estimators': [200,300,400], 'max_features':[8,10,12,14]},\n",
    "    {'bootstrap': [False] ,'n_estimators': [200,400], 'max_features':[4,8,12]},\n",
    "\n",
    "]\n",
    "grid_search = GridSearchCV(clf,param_grid,cv = 5, scoring = \"accuracy\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "# total of (3x4 + 3 x 2) * 5 = 90 trains\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - random_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = RandomForestClassifier(n_estimators= 300, max_features=8, bootstrap= True)\n",
    "clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "c_matrix =confusion_matrix(y_test,preds)\n",
    "sns.heatmap(c_matrix.T,square= True, annot= True, fmt= 'd', xticklabels = [\"No\",\"Yes\"],yticklabels=[\"No\",\"Yes\"])\n",
    "plt.xlabel('Test Values')\n",
    "plt.ylabel('Predictions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test,preds,output_dict=True,target_names= [\"No\",\"Yes\"])\n",
    "df = pd.DataFrame(report).transpose()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allY = pd.DataFrame(data = y_test)\n",
    "allY[\"Predicted\"] = preds\n",
    "allY.rename(columns= {\"RainTomorrow\":\"Actual\"}, inplace=True)\n",
    "incorrect = allY[allY[\"Actual\"] != allY[\"Predicted\"]]\n",
    "incorrect_index = incorrect.index\n",
    "incorrect_rows = weatherAUS.iloc[incorrect_index]\n",
    "incorrect_rows.T"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
